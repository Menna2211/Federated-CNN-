{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULr0-TOM6FPV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6eb77585-85fc-49d9-d9d8-6ac39667106c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUASjoeTgeRH"
      },
      "source": [
        "# Standard scientific Python imports and Import  classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0DE_9Grwhpd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7a5836c8-3723-4c94-b537-dac093383b48"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import linalg\n",
        "from numpy.linalg import norm\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "from imutils import paths\n",
        "#We import sklearn.\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import backend as K\n",
        "from keras.regularizers import l2\n",
        "\n",
        "\n",
        "# We'll use matplotlib for graphics.\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2Gik21ZgMDG"
      },
      "source": [
        "# Loading training and testing data (MINST)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTdKcyI95lPB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "1c21636c-99a9-4aa1-e734-0962396fd68a"
      },
      "source": [
        "# Load data.\n",
        "(X_train , y_train),(X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test  = X_test.astype('float32')\n",
        "X_train /= 255 # Original data is uint8 (0-255). Scale it to range [0,1].\n",
        "X_test  /= 255\n",
        "\n",
        "#binarize the labels\n",
        "lb = LabelBinarizer()\n",
        "y_train = lb.fit_transform(y_train)\n",
        "y_test = lb.fit_transform(y_test)\n",
        "\n",
        "print(\"Training X matrix shape\", X_train.shape)\n",
        "print('---------------------------------------- ')\n",
        "print(\"Testing X matrix shape\", X_test.shape)\n",
        "print('---------------------------------------- ')\n",
        "print('y Train Shape is : ' , y_train.shape)\n",
        "print('---------------------------------------- ')\n",
        "print('y Test Shape is : ' , y_test.shape)\n",
        "print('---------------------------------------- ')\n",
        "print('All y is : ' , np.unique( y_train ))\n",
        "\n",
        "plt.figure(figsize=(12,10))\n",
        "plt.style.use('ggplot')\n",
        "for i in  range(5)  :\n",
        "    plt.subplot(1,5,i+1)\n",
        "    plt.imshow(X_train[i].reshape(28,28), cmap='gray', interpolation='nearest')\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training X matrix shape (60000, 28, 28, 1)\n",
            "---------------------------------------- \n",
            "Testing X matrix shape (10000, 28, 28, 1)\n",
            "---------------------------------------- \n",
            "y Train Shape is :  (60000, 10)\n",
            "---------------------------------------- \n",
            "y Test Shape is :  (10000, 10)\n",
            "---------------------------------------- \n",
            "All y is :  [0 1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x720 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_tN1nX9f0t-"
      },
      "source": [
        "# Creating clients and batching training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9JftlSfwmlj"
      },
      "source": [
        "def create_clients(image_list, label_list, num_clients, initial='clients'):\n",
        "    ''' return: a dictionary with keys clients' names and value as\n",
        "                data shards - tuple of images and label lists.\n",
        "        args:\n",
        "            image_list: a list of numpy arrays of training images\n",
        "            label_list:a list of binarized labels for each image\n",
        "            num_client: number of fedrated members (clients)\n",
        "            initials: the clients'name prefix, e.g, clients_1\n",
        "\n",
        "    '''\n",
        "\n",
        "    #create a list of client names\n",
        "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
        "\n",
        "    #randomize the data\n",
        "    data = list(zip(image_list, label_list))\n",
        "    random.shuffle(data)\n",
        "\n",
        "    #shard data and place at each client\n",
        "    size = len(data)//num_clients\n",
        "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
        "\n",
        "    #number of clients must equal number of shards\n",
        "    assert(len(shards) == len(client_names))\n",
        "\n",
        "    return {client_names[i] : shards[i] for i in range(len(client_names))}\n",
        "\n",
        "def batch_data(data_shard, batch_size):\n",
        "    '''Takes in a clients data shard and create a tfds object off it\n",
        "    args:\n",
        "        shard: a data, label constituting a client's data shard\n",
        "        bs:batch size\n",
        "    return:\n",
        "        tfds object'''\n",
        "    #seperate shard into data and labels lists\n",
        "    data, label = zip(*data_shard)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
        "    return dataset.shuffle(len(label)).batch(batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2SC8vXo7nwh"
      },
      "source": [
        "#create clients\n",
        "comms_round = 100\n",
        "n_clients = 10\n",
        "batch_size = 128\n",
        "clients = create_clients(X_train, y_train, n_clients, initial='client')\n",
        "\n",
        "#process and batch the training data for each client\n",
        "clients_batched = dict()\n",
        "for (client_name, data) in clients.items():\n",
        "    clients_batched[client_name] = batch_data(data, batch_size)\n",
        "\n",
        "#process and batch the test set\n",
        "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCEJ-PmA8YRY"
      },
      "source": [
        "# Creating a model with Keras\n",
        "\n",
        "* INPUT: 28 × 28 × 1\n",
        "* CONV5: 5 × 5 size, 32 filters, 1 stride\n",
        "* ReLU: max(0,hθ(x))\n",
        "* POOL: 2 × 2 size, 1 stride\n",
        "* CONV5: 5 × 5 size, 64 filters, 1 stride\n",
        "* ReLU: max(0,hθ(x))\n",
        "* POOL: 2 × 2 size, 1 stride\n",
        "* FC: 1024 Hidden Neurons\n",
        "* DROPOUT: p = 0.5\n",
        "* FC: 10 Output Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK1Bdwck2klE"
      },
      "source": [
        "lr = 0.02\n",
        "loss='categorical_crossentropy'\n",
        "#'squared_hinge'\n",
        "#'categorical_crossentropy'\n",
        "metrics = ['accuracy']\n",
        "optimizer = SGD(lr=lr,\n",
        "                decay=lr / comms_round,\n",
        "                momentum=0.9\n",
        "               )\n",
        "\n",
        "\n",
        "class SimpleMLP:\n",
        "    @staticmethod\n",
        "    def build(shape, classes):\n",
        "        model = Sequential()\n",
        "        ############################\n",
        "        model.add(tf.keras.layers.Conv2D(32, (5,5), input_shape=shape, activation='relu'))\n",
        "        model.add(tf.keras.layers.MaxPool2D((2, 2)))\n",
        "        model.add(tf.keras.layers.Conv2D(64, (5,5), activation='relu'))\n",
        "        model.add(tf.keras.layers.MaxPool2D((2, 2)))\n",
        "        model.add(tf.keras.layers.Flatten())\n",
        "        model.add(Dense(1028, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        #############################\n",
        "        model.add(Dense(classes ))\n",
        "        model.add(Activation(\"softmax\"))\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywVegJ1ClNkK"
      },
      "source": [
        "# Model Aggregation (Federated Averaging)\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8uvmfM34iKE"
      },
      "source": [
        "def weight_scalling_factor(clients_trn_data, client_name):\n",
        "    client_names = list(clients_trn_data.keys())\n",
        "    #get the bs\n",
        "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
        "    #first calculate the total training data points across clinets\n",
        "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
        "    # get the total number of data points held by a client\n",
        "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
        "    return local_count/global_count\n",
        "\n",
        "\n",
        "def scale_model_weights(weight, scalar):\n",
        "    '''function for scaling a models weights'''\n",
        "    weight_final = []\n",
        "    steps = len(weight)\n",
        "    for i in range(steps):\n",
        "        weight_final.append(scalar * weight[i])\n",
        "    return weight_final\n",
        "\n",
        "\n",
        "\n",
        "def sum_scaled_weights(scaled_weight_list):\n",
        "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
        "    avg_grad = list()\n",
        "    #get the average grad accross all client gradients\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
        "        avg_grad.append(layer_mean)\n",
        "\n",
        "    return avg_grad\n",
        "\n",
        "\n",
        "def test_model(X_test, Y_test,  model, comm_round):\n",
        "    #cce = tf.keras.losses.SquaredHinge()\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    #logits = model.predict(X_test, batch_size=100)\n",
        "    y_pred = model.predict(X_test)\n",
        "    loss = cce(Y_test, y_pred)\n",
        "    acc = accuracy_score(tf.argmax(y_pred, axis=1), tf.argmax(Y_test, axis=1))\n",
        "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
        "    return acc, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOUNcPjCqFyr"
      },
      "source": [
        "# Federated Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZVjawfo43fO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3cacdf7c-5399-40b8-bc26-072d47c7e964"
      },
      "source": [
        "#initialize global model\n",
        "smlp_global = SimpleMLP()\n",
        "global_model = smlp_global.build(input_shape, 10)\n",
        "\n",
        "#commence global training loop\n",
        "for comm_round in range(comms_round):\n",
        "\n",
        "    # get the global model's weights - will serve as the initial weights for all local models\n",
        "    global_weights = global_model.get_weights()\n",
        "\n",
        "    #initial list to collect local model weights after scalling\n",
        "    scaled_local_weight_list = list()\n",
        "\n",
        "    #randomize client data - using keys\n",
        "    client_names= list(clients_batched.keys())\n",
        "    random.shuffle(client_names)\n",
        "\n",
        "    #loop through each client and create new local model\n",
        "    for client in client_names:\n",
        "        smlp_local = SimpleMLP()\n",
        "        #local_model = smlp_local.build(784, 10)\n",
        "        local_model = smlp_local.build(input_shape, 10)\n",
        "        local_model.compile(loss=loss,\n",
        "                      optimizer=optimizer,\n",
        "                      metrics=metrics)\n",
        "\n",
        "        #set local model weight to the weight of the global model\n",
        "        local_model.set_weights(global_weights)\n",
        "\n",
        "        #fit local model with client's data\n",
        "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
        "\n",
        "        #scale the model weights and add to list\n",
        "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
        "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
        "        scaled_local_weight_list.append(scaled_weights)\n",
        "\n",
        "        #clear session to free memory after each communication round\n",
        "        K.clear_session()\n",
        "\n",
        "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
        "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
        "\n",
        "    #update global model\n",
        "    global_model.set_weights(average_weights)\n",
        "\n",
        "    #test global model and print out metrics after each communications round\n",
        "    for(X_test, y_test) in test_batched:\n",
        "        global_acc, global_loss = test_model(X_test, y_test, global_model, comm_round)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "comm_round: 0 | global_acc: 90.110% | global_loss: 1.6409269571304321\n",
            "comm_round: 1 | global_acc: 94.590% | global_loss: 1.551819920539856\n",
            "comm_round: 2 | global_acc: 95.930% | global_loss: 1.5289334058761597\n",
            "comm_round: 3 | global_acc: 96.780% | global_loss: 1.5160949230194092\n",
            "comm_round: 4 | global_acc: 97.110% | global_loss: 1.5091586112976074\n",
            "comm_round: 5 | global_acc: 97.420% | global_loss: 1.5046546459197998\n",
            "comm_round: 6 | global_acc: 97.660% | global_loss: 1.4998914003372192\n",
            "comm_round: 7 | global_acc: 97.820% | global_loss: 1.4975299835205078\n",
            "comm_round: 8 | global_acc: 97.910% | global_loss: 1.4947277307510376\n",
            "comm_round: 9 | global_acc: 98.110% | global_loss: 1.4931905269622803\n",
            "comm_round: 10 | global_acc: 98.140% | global_loss: 1.4917436838150024\n",
            "comm_round: 11 | global_acc: 98.160% | global_loss: 1.4905918836593628\n",
            "comm_round: 12 | global_acc: 98.310% | global_loss: 1.4890528917312622\n",
            "comm_round: 13 | global_acc: 98.430% | global_loss: 1.487945556640625\n",
            "comm_round: 14 | global_acc: 98.450% | global_loss: 1.4871728420257568\n",
            "comm_round: 15 | global_acc: 98.540% | global_loss: 1.486530065536499\n",
            "comm_round: 16 | global_acc: 98.620% | global_loss: 1.485879898071289\n",
            "comm_round: 17 | global_acc: 98.620% | global_loss: 1.4856085777282715\n",
            "comm_round: 18 | global_acc: 98.670% | global_loss: 1.4844061136245728\n",
            "comm_round: 19 | global_acc: 98.680% | global_loss: 1.4838300943374634\n",
            "comm_round: 20 | global_acc: 98.690% | global_loss: 1.4835511445999146\n",
            "comm_round: 21 | global_acc: 98.720% | global_loss: 1.4832260608673096\n",
            "comm_round: 22 | global_acc: 98.710% | global_loss: 1.4830303192138672\n",
            "comm_round: 23 | global_acc: 98.760% | global_loss: 1.482672929763794\n",
            "comm_round: 24 | global_acc: 98.770% | global_loss: 1.4822509288787842\n",
            "comm_round: 25 | global_acc: 98.760% | global_loss: 1.482077717781067\n",
            "comm_round: 26 | global_acc: 98.790% | global_loss: 1.4813947677612305\n",
            "comm_round: 27 | global_acc: 98.800% | global_loss: 1.4812339544296265\n",
            "comm_round: 28 | global_acc: 98.770% | global_loss: 1.48099684715271\n",
            "comm_round: 29 | global_acc: 98.780% | global_loss: 1.4807682037353516\n",
            "comm_round: 30 | global_acc: 98.800% | global_loss: 1.4807698726654053\n",
            "comm_round: 31 | global_acc: 98.850% | global_loss: 1.4802978038787842\n",
            "comm_round: 32 | global_acc: 98.860% | global_loss: 1.480082392692566\n",
            "comm_round: 33 | global_acc: 98.840% | global_loss: 1.4799994230270386\n",
            "comm_round: 34 | global_acc: 98.870% | global_loss: 1.4797908067703247\n",
            "comm_round: 35 | global_acc: 98.860% | global_loss: 1.4795899391174316\n",
            "comm_round: 36 | global_acc: 98.870% | global_loss: 1.4794526100158691\n",
            "comm_round: 37 | global_acc: 98.850% | global_loss: 1.479338526725769\n",
            "comm_round: 38 | global_acc: 98.880% | global_loss: 1.479060411453247\n",
            "comm_round: 39 | global_acc: 98.840% | global_loss: 1.4790825843811035\n",
            "comm_round: 40 | global_acc: 98.860% | global_loss: 1.4789257049560547\n",
            "comm_round: 41 | global_acc: 98.870% | global_loss: 1.4786328077316284\n",
            "comm_round: 42 | global_acc: 98.890% | global_loss: 1.478453278541565\n",
            "comm_round: 43 | global_acc: 98.910% | global_loss: 1.478395938873291\n",
            "comm_round: 44 | global_acc: 98.890% | global_loss: 1.478379249572754\n",
            "comm_round: 45 | global_acc: 98.890% | global_loss: 1.4781622886657715\n",
            "comm_round: 46 | global_acc: 98.880% | global_loss: 1.4782253503799438\n",
            "comm_round: 47 | global_acc: 98.900% | global_loss: 1.4780833721160889\n",
            "comm_round: 48 | global_acc: 98.900% | global_loss: 1.478041648864746\n",
            "comm_round: 49 | global_acc: 98.880% | global_loss: 1.478005051612854\n",
            "comm_round: 50 | global_acc: 98.920% | global_loss: 1.4777289628982544\n",
            "comm_round: 51 | global_acc: 98.910% | global_loss: 1.477612853050232\n",
            "comm_round: 52 | global_acc: 98.920% | global_loss: 1.4776173830032349\n",
            "comm_round: 53 | global_acc: 98.960% | global_loss: 1.4776220321655273\n",
            "comm_round: 54 | global_acc: 98.920% | global_loss: 1.4774885177612305\n",
            "comm_round: 55 | global_acc: 98.930% | global_loss: 1.4773030281066895\n",
            "comm_round: 56 | global_acc: 98.950% | global_loss: 1.4772446155548096\n",
            "comm_round: 57 | global_acc: 98.920% | global_loss: 1.4771881103515625\n",
            "comm_round: 58 | global_acc: 98.930% | global_loss: 1.477244257926941\n",
            "comm_round: 59 | global_acc: 98.940% | global_loss: 1.4770184755325317\n",
            "comm_round: 60 | global_acc: 98.920% | global_loss: 1.4772061109542847\n",
            "comm_round: 61 | global_acc: 98.940% | global_loss: 1.4769433736801147\n",
            "comm_round: 62 | global_acc: 98.940% | global_loss: 1.4768677949905396\n",
            "comm_round: 63 | global_acc: 98.950% | global_loss: 1.4769788980484009\n",
            "comm_round: 64 | global_acc: 98.940% | global_loss: 1.476745367050171\n",
            "comm_round: 65 | global_acc: 98.990% | global_loss: 1.4767125844955444\n",
            "comm_round: 66 | global_acc: 98.940% | global_loss: 1.4765568971633911\n",
            "comm_round: 67 | global_acc: 98.940% | global_loss: 1.4764760732650757\n",
            "comm_round: 68 | global_acc: 98.970% | global_loss: 1.476470947265625\n",
            "comm_round: 69 | global_acc: 98.970% | global_loss: 1.4765743017196655\n",
            "comm_round: 70 | global_acc: 98.970% | global_loss: 1.4765347242355347\n",
            "comm_round: 71 | global_acc: 99.000% | global_loss: 1.4765070676803589\n",
            "comm_round: 72 | global_acc: 98.930% | global_loss: 1.4763866662979126\n",
            "comm_round: 73 | global_acc: 98.980% | global_loss: 1.4763935804367065\n",
            "comm_round: 74 | global_acc: 98.980% | global_loss: 1.4762425422668457\n",
            "comm_round: 75 | global_acc: 98.990% | global_loss: 1.4763450622558594\n",
            "comm_round: 76 | global_acc: 98.970% | global_loss: 1.4761180877685547\n",
            "comm_round: 77 | global_acc: 98.980% | global_loss: 1.476028561592102\n",
            "comm_round: 78 | global_acc: 98.970% | global_loss: 1.4760690927505493\n",
            "comm_round: 79 | global_acc: 98.980% | global_loss: 1.4760369062423706\n",
            "comm_round: 80 | global_acc: 99.000% | global_loss: 1.4760277271270752\n",
            "comm_round: 81 | global_acc: 98.980% | global_loss: 1.475985050201416\n",
            "comm_round: 82 | global_acc: 99.020% | global_loss: 1.4758304357528687\n",
            "comm_round: 83 | global_acc: 98.980% | global_loss: 1.4759316444396973\n",
            "comm_round: 84 | global_acc: 99.000% | global_loss: 1.475810170173645\n",
            "comm_round: 85 | global_acc: 98.980% | global_loss: 1.4759713411331177\n",
            "comm_round: 86 | global_acc: 98.990% | global_loss: 1.4758168458938599\n",
            "comm_round: 87 | global_acc: 99.020% | global_loss: 1.4757260084152222\n",
            "comm_round: 88 | global_acc: 98.990% | global_loss: 1.4757328033447266\n",
            "comm_round: 89 | global_acc: 99.000% | global_loss: 1.4757099151611328\n",
            "comm_round: 90 | global_acc: 98.980% | global_loss: 1.4756064414978027\n",
            "comm_round: 91 | global_acc: 98.980% | global_loss: 1.47561776638031\n",
            "comm_round: 92 | global_acc: 99.040% | global_loss: 1.4755862951278687\n",
            "comm_round: 93 | global_acc: 99.020% | global_loss: 1.4756240844726562\n",
            "comm_round: 94 | global_acc: 99.000% | global_loss: 1.4756169319152832\n",
            "comm_round: 95 | global_acc: 99.030% | global_loss: 1.4755133390426636\n",
            "comm_round: 96 | global_acc: 99.010% | global_loss: 1.4754353761672974\n",
            "comm_round: 97 | global_acc: 99.060% | global_loss: 1.475487470626831\n",
            "comm_round: 98 | global_acc: 99.030% | global_loss: 1.475413203239441\n",
            "comm_round: 99 | global_acc: 98.990% | global_loss: 1.475319266319275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhBa0U38_2NA"
      },
      "source": [
        "score1=global_model.evaluate(X_test , y_test , batch_size=batch_size , verbose= 1)\n",
        "print('Test_acc: {:.3%} | Test_loss: {}'.format( score1[1], score1[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89f5Qi8GrQyw"
      },
      "source": [
        "# standard SGD model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEyybwUZ5iPL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3a43c1d5-f5fd-43a9-9215-ec29be84a357"
      },
      "source": [
        "SGD_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(y_train)).batch(batch_size)\n",
        "smlp_SGD = SimpleMLP()\n",
        "SGD_model = smlp_SGD.build(input_shape, 10)\n",
        "\n",
        "SGD_model.compile(loss=loss,\n",
        "              optimizer=optimizer,\n",
        "              metrics=metrics)\n",
        "\n",
        "# fit the SGD training data to model\n",
        "_ = SGD_model.fit(SGD_dataset, epochs=comms_round, verbose=1 , validation_data=(X_test, y_test))\n",
        "\n",
        "#test the SGD global model and print out metrics\n",
        "#for(X_test, y_test) in test_batched:\n",
        "#        SGD_acc, SGD_loss = test_model(X_test, y_test, SGD_model, 1)\n",
        "\n",
        "score=SGD_model.evaluate(X_test , y_test , batch_size=batch_size , verbose= 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.8366 - accuracy: 0.7455 - val_loss: 0.2174 - val_accuracy: 0.9362\n",
            "Epoch 2/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2159 - accuracy: 0.9348 - val_loss: 0.1337 - val_accuracy: 0.9604\n",
            "Epoch 3/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1522 - accuracy: 0.9542 - val_loss: 0.0965 - val_accuracy: 0.9716\n",
            "Epoch 4/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1209 - accuracy: 0.9635 - val_loss: 0.0795 - val_accuracy: 0.9760\n",
            "Epoch 5/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1024 - accuracy: 0.9689 - val_loss: 0.0679 - val_accuracy: 0.9790\n",
            "Epoch 6/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0917 - accuracy: 0.9724 - val_loss: 0.0604 - val_accuracy: 0.9810\n",
            "Epoch 7/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0823 - accuracy: 0.9751 - val_loss: 0.0575 - val_accuracy: 0.9813\n",
            "Epoch 8/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0752 - accuracy: 0.9769 - val_loss: 0.0501 - val_accuracy: 0.9848\n",
            "Epoch 9/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0687 - accuracy: 0.9791 - val_loss: 0.0487 - val_accuracy: 0.9843\n",
            "Epoch 10/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0646 - accuracy: 0.9801 - val_loss: 0.0439 - val_accuracy: 0.9864\n",
            "Epoch 11/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0602 - accuracy: 0.9816 - val_loss: 0.0413 - val_accuracy: 0.9877\n",
            "Epoch 12/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0568 - accuracy: 0.9824 - val_loss: 0.0412 - val_accuracy: 0.9871\n",
            "Epoch 13/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0534 - accuracy: 0.9845 - val_loss: 0.0391 - val_accuracy: 0.9879\n",
            "Epoch 14/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0510 - accuracy: 0.9842 - val_loss: 0.0363 - val_accuracy: 0.9882\n",
            "Epoch 15/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0482 - accuracy: 0.9851 - val_loss: 0.0361 - val_accuracy: 0.9880\n",
            "Epoch 16/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0473 - accuracy: 0.9856 - val_loss: 0.0336 - val_accuracy: 0.9889\n",
            "Epoch 17/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0440 - accuracy: 0.9864 - val_loss: 0.0340 - val_accuracy: 0.9879\n",
            "Epoch 18/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0428 - accuracy: 0.9868 - val_loss: 0.0334 - val_accuracy: 0.9889\n",
            "Epoch 19/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0415 - accuracy: 0.9870 - val_loss: 0.0312 - val_accuracy: 0.9903\n",
            "Epoch 20/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0382 - accuracy: 0.9885 - val_loss: 0.0327 - val_accuracy: 0.9895\n",
            "Epoch 21/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0378 - accuracy: 0.9885 - val_loss: 0.0299 - val_accuracy: 0.9901\n",
            "Epoch 22/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0379 - accuracy: 0.9883 - val_loss: 0.0299 - val_accuracy: 0.9894\n",
            "Epoch 23/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0350 - accuracy: 0.9891 - val_loss: 0.0292 - val_accuracy: 0.9900\n",
            "Epoch 24/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0342 - accuracy: 0.9897 - val_loss: 0.0281 - val_accuracy: 0.9905\n",
            "Epoch 25/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0334 - accuracy: 0.9897 - val_loss: 0.0300 - val_accuracy: 0.9895\n",
            "Epoch 26/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0321 - accuracy: 0.9898 - val_loss: 0.0274 - val_accuracy: 0.9902\n",
            "Epoch 27/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0319 - accuracy: 0.9896 - val_loss: 0.0286 - val_accuracy: 0.9904\n",
            "Epoch 28/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0302 - accuracy: 0.9905 - val_loss: 0.0272 - val_accuracy: 0.9909\n",
            "Epoch 29/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0294 - accuracy: 0.9906 - val_loss: 0.0275 - val_accuracy: 0.9905\n",
            "Epoch 30/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0292 - accuracy: 0.9912 - val_loss: 0.0260 - val_accuracy: 0.9910\n",
            "Epoch 31/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0280 - accuracy: 0.9915 - val_loss: 0.0268 - val_accuracy: 0.9907\n",
            "Epoch 32/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0271 - accuracy: 0.9916 - val_loss: 0.0266 - val_accuracy: 0.9912\n",
            "Epoch 33/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0272 - accuracy: 0.9913 - val_loss: 0.0263 - val_accuracy: 0.9907\n",
            "Epoch 34/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0261 - accuracy: 0.9922 - val_loss: 0.0269 - val_accuracy: 0.9908\n",
            "Epoch 35/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0240 - accuracy: 0.9926 - val_loss: 0.0262 - val_accuracy: 0.9911\n",
            "Epoch 36/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0245 - accuracy: 0.9925 - val_loss: 0.0258 - val_accuracy: 0.9914\n",
            "Epoch 37/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0246 - accuracy: 0.9924 - val_loss: 0.0248 - val_accuracy: 0.9912\n",
            "Epoch 38/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0241 - accuracy: 0.9924 - val_loss: 0.0250 - val_accuracy: 0.9912\n",
            "Epoch 39/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0231 - accuracy: 0.9929 - val_loss: 0.0249 - val_accuracy: 0.9911\n",
            "Epoch 40/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0218 - accuracy: 0.9932 - val_loss: 0.0258 - val_accuracy: 0.9913\n",
            "Epoch 41/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0220 - accuracy: 0.9932 - val_loss: 0.0259 - val_accuracy: 0.9911\n",
            "Epoch 42/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0195 - accuracy: 0.9941 - val_loss: 0.0244 - val_accuracy: 0.9912\n",
            "Epoch 43/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0210 - accuracy: 0.9935 - val_loss: 0.0247 - val_accuracy: 0.9914\n",
            "Epoch 44/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0193 - accuracy: 0.9943 - val_loss: 0.0242 - val_accuracy: 0.9917\n",
            "Epoch 45/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0204 - accuracy: 0.9941 - val_loss: 0.0240 - val_accuracy: 0.9912\n",
            "Epoch 46/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0196 - accuracy: 0.9937 - val_loss: 0.0244 - val_accuracy: 0.9911\n",
            "Epoch 47/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0195 - accuracy: 0.9941 - val_loss: 0.0240 - val_accuracy: 0.9918\n",
            "Epoch 48/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0194 - accuracy: 0.9938 - val_loss: 0.0236 - val_accuracy: 0.9921\n",
            "Epoch 49/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0188 - accuracy: 0.9941 - val_loss: 0.0237 - val_accuracy: 0.9919\n",
            "Epoch 50/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0183 - accuracy: 0.9942 - val_loss: 0.0236 - val_accuracy: 0.9916\n",
            "Epoch 51/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0174 - accuracy: 0.9948 - val_loss: 0.0235 - val_accuracy: 0.9920\n",
            "Epoch 52/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0172 - accuracy: 0.9948 - val_loss: 0.0238 - val_accuracy: 0.9920\n",
            "Epoch 53/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0171 - accuracy: 0.9947 - val_loss: 0.0239 - val_accuracy: 0.9915\n",
            "Epoch 54/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0165 - accuracy: 0.9946 - val_loss: 0.0232 - val_accuracy: 0.9917\n",
            "Epoch 55/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0164 - accuracy: 0.9949 - val_loss: 0.0237 - val_accuracy: 0.9913\n",
            "Epoch 56/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0155 - accuracy: 0.9952 - val_loss: 0.0238 - val_accuracy: 0.9919\n",
            "Epoch 57/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0156 - accuracy: 0.9952 - val_loss: 0.0237 - val_accuracy: 0.9912\n",
            "Epoch 58/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0152 - accuracy: 0.9954 - val_loss: 0.0241 - val_accuracy: 0.9913\n",
            "Epoch 59/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0151 - accuracy: 0.9954 - val_loss: 0.0234 - val_accuracy: 0.9918\n",
            "Epoch 60/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0149 - accuracy: 0.9955 - val_loss: 0.0239 - val_accuracy: 0.9915\n",
            "Epoch 61/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0145 - accuracy: 0.9955 - val_loss: 0.0230 - val_accuracy: 0.9920\n",
            "Epoch 62/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0152 - accuracy: 0.9954 - val_loss: 0.0233 - val_accuracy: 0.9920\n",
            "Epoch 63/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0141 - accuracy: 0.9962 - val_loss: 0.0237 - val_accuracy: 0.9923\n",
            "Epoch 64/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0148 - accuracy: 0.9954 - val_loss: 0.0234 - val_accuracy: 0.9919\n",
            "Epoch 65/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0136 - accuracy: 0.9956 - val_loss: 0.0223 - val_accuracy: 0.9917\n",
            "Epoch 66/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0130 - accuracy: 0.9962 - val_loss: 0.0223 - val_accuracy: 0.9923\n",
            "Epoch 67/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0129 - accuracy: 0.9960 - val_loss: 0.0226 - val_accuracy: 0.9924\n",
            "Epoch 68/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0134 - accuracy: 0.9959 - val_loss: 0.0230 - val_accuracy: 0.9924\n",
            "Epoch 69/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0125 - accuracy: 0.9964 - val_loss: 0.0234 - val_accuracy: 0.9918\n",
            "Epoch 70/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0127 - accuracy: 0.9960 - val_loss: 0.0223 - val_accuracy: 0.9923\n",
            "Epoch 71/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0121 - accuracy: 0.9966 - val_loss: 0.0222 - val_accuracy: 0.9924\n",
            "Epoch 72/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0121 - accuracy: 0.9962 - val_loss: 0.0231 - val_accuracy: 0.9920\n",
            "Epoch 73/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0116 - accuracy: 0.9966 - val_loss: 0.0230 - val_accuracy: 0.9919\n",
            "Epoch 74/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0114 - accuracy: 0.9964 - val_loss: 0.0234 - val_accuracy: 0.9915\n",
            "Epoch 75/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0112 - accuracy: 0.9964 - val_loss: 0.0228 - val_accuracy: 0.9919\n",
            "Epoch 76/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0113 - accuracy: 0.9969 - val_loss: 0.0227 - val_accuracy: 0.9920\n",
            "Epoch 77/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0114 - accuracy: 0.9963 - val_loss: 0.0223 - val_accuracy: 0.9924\n",
            "Epoch 78/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0114 - accuracy: 0.9965 - val_loss: 0.0240 - val_accuracy: 0.9918\n",
            "Epoch 79/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0115 - accuracy: 0.9965 - val_loss: 0.0221 - val_accuracy: 0.9923\n",
            "Epoch 80/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0104 - accuracy: 0.9968 - val_loss: 0.0221 - val_accuracy: 0.9923\n",
            "Epoch 81/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0103 - accuracy: 0.9969 - val_loss: 0.0227 - val_accuracy: 0.9918\n",
            "Epoch 82/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0103 - accuracy: 0.9970 - val_loss: 0.0219 - val_accuracy: 0.9919\n",
            "Epoch 83/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0102 - accuracy: 0.9970 - val_loss: 0.0222 - val_accuracy: 0.9923\n",
            "Epoch 84/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0101 - accuracy: 0.9970 - val_loss: 0.0222 - val_accuracy: 0.9920\n",
            "Epoch 85/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0101 - accuracy: 0.9969 - val_loss: 0.0224 - val_accuracy: 0.9924\n",
            "Epoch 86/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0101 - accuracy: 0.9970 - val_loss: 0.0232 - val_accuracy: 0.9921\n",
            "Epoch 87/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0098 - accuracy: 0.9971 - val_loss: 0.0221 - val_accuracy: 0.9921\n",
            "Epoch 88/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0096 - accuracy: 0.9970 - val_loss: 0.0232 - val_accuracy: 0.9918\n",
            "Epoch 89/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0094 - accuracy: 0.9974 - val_loss: 0.0229 - val_accuracy: 0.9921\n",
            "Epoch 90/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0089 - accuracy: 0.9975 - val_loss: 0.0232 - val_accuracy: 0.9922\n",
            "Epoch 91/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0098 - accuracy: 0.9970 - val_loss: 0.0225 - val_accuracy: 0.9924\n",
            "Epoch 92/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0096 - accuracy: 0.9970 - val_loss: 0.0228 - val_accuracy: 0.9925\n",
            "Epoch 93/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0096 - accuracy: 0.9971 - val_loss: 0.0231 - val_accuracy: 0.9916\n",
            "Epoch 94/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0087 - accuracy: 0.9974 - val_loss: 0.0233 - val_accuracy: 0.9921\n",
            "Epoch 95/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0088 - accuracy: 0.9975 - val_loss: 0.0237 - val_accuracy: 0.9925\n",
            "Epoch 96/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0086 - accuracy: 0.9976 - val_loss: 0.0225 - val_accuracy: 0.9923\n",
            "Epoch 97/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0089 - accuracy: 0.9973 - val_loss: 0.0223 - val_accuracy: 0.9925\n",
            "Epoch 98/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.0229 - val_accuracy: 0.9921\n",
            "Epoch 99/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0082 - accuracy: 0.9976 - val_loss: 0.0226 - val_accuracy: 0.9924\n",
            "Epoch 100/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0081 - accuracy: 0.9977 - val_loss: 0.0230 - val_accuracy: 0.9921\n",
            "79/79 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.9921\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSxwwGbP_Lhf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bb68db51-d971-488d-8f76-e0a6b55e7f10"
      },
      "source": [
        "print('Test_acc: {:.3%} | Test_loss: {}'.format( score[1], score[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test_acc: 99.210% | Test_loss: 0.022951742634177208\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}